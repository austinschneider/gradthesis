\chapter{Conclusions and next steps\label{chapter:conclusions}}
\section{Analysis conclusions\label{sec:analysis_conclusions}}
\begingroup
\graphicspath{{results/HESE_Final_Paper/}}
\input{results/HESE_Final_Paper/sections/conclusions}
\endgroup
\FloatBarrier

\section{The next generation}
The analysis presented here updates previous studies of the HESE sample and is only a small part of the picture for astrophysical neutrinos and the mystery of cosmic ray origin.
However, the techniques and solutions developed here remain broadly applicable to future analyses and will enable more precise measurements as we accumulate more data, and develop additional samples.
This section explores what the next generation of analyses may look like and the possible avenues to explore beyond the work presented here.

The next objectives for analyzing the astrophysical neutrino flux are to expand the energy range and flavor information of the analysis and simultaneously examine these different channels for additional structure or differences between them.
Some analyses have already examined these channels, for instance, samples that look at muon neutrinos from the Northern hemisphere between $\SI{100}\GeV$ and $\SI{10}\PeV$ or cascade focused samples that extend down to $\sim\SI{1}\TeV$.
However, the separate analyses of these samples have either lacked extended systematics treatments, lacked enough data to make significant conclusions, or are too disparate for meaningful comparison between each other.
Further investigation of the astrophysical neutrino flux will require the simultaneous analysis of all these data samples and expand upon the techniques developed for this work.
Many of these samples were developed for the initial measurement of the astrophysical neutrino.
However, as more data becomes available, the precision of these analyses will no longer rely on maximizing the selection's acceptance.
Instead, we will be able to focus on the analysis accuracy in an attempt to understand the differences we have observed between the various astrophysical neutrino measurement channels, even if this comes at the cost of some precision.

\subsection{Data samples}
The cascade data channel can achieve low background rates in all directions because it is unlikely for muons to produce cascade-like detector signatures, even at lower energies.
The HESE selection as a cascade-dominated sample takes advantage of this in part but cannot cover lower energies.
Additional selection techniques can obtain a high purity sample of cascade-like neutrino events that extends down to $\SI{1}\TeV$.
Two IceCube event selections have achieved this, namely, the ``medium-energy starting event selection'' (MESE) and the ``cascade selection''.
These cascade dominated selections are largely comprised of $NC$, $\nu_e$ CC, and a sub-population of $\nu_\tau$ CC events.
In the MESE selection, some $\nu_\mu$ CC events are also present in the sample.
These samples cover the entire sky, enabling the combined $\nu_e$ and $\nu_\tau$ flux to be accurately measured in both hemispheres, although finer grained angular studies are limited by poor directional resolution.
A major advantage of these samples is the higher energy resolution of cascade events, enabling more detailed studies of the neutrino energy spectrum.

Beyond cascades, we hope to incorporate similar information from $\nu_\mu$ CC interactions.
This is available through the different track dominated event selections.
Two IceCube selections use a similar approach to obtain a sample of high-quality through-going muon neutrino tracks, namely by restricting selected events to tracks known to be up-going.
These are the selection used for the Northern hemisphere astrophysical neutrino flux measurement, and the selection used in the search for $\si\eV$ scale sterile neutrinos.
Between these two selections, the primary difference lies in the method of selection cuts; the astrophysical measurement sample uses a boosted decision tree, while the sterile search sample uses only hand tunes straight cuts.
This choice stems from a difference in motivation for the samples.
The astrophysical sample aims to maximize effective area and angular range, while the sterile search sample aims for the high-quality reconstruction and maximum data-simulation agreement.
These samples only cover the Northern hemisphere but enable constraints on the energy and angular distribution of muon neutrinos within that range.
One disadvantage of these samples is their limited energy resolution that stems from the challenges of inferring muon energies.
Even assuming an isotropic 1:1:1 astrophysical flux, there are benefits to including one of these samples in the analysis.
These samples extend between $\SI{100}\GeV$ and $\SI{10}\PeV$, and below a reconstructed energy of $\sim\SI{20}\TeV$ atmospheric neutrinos dominate the flux.
This large population of atmospheric neutrinos can help to pin down the unknown values of the atmospheric systematic parameters, thereby reducing uncertainty in the astrophysical measurements even for other samples.

To examine $\nu_\mu$ CC events in the Southern hemisphere at lower energies ($<\SI{100}\TeV$) a severe suppression of the overwhelming muon background is required.
The enhanced starting track event selection (ESTES) fills this role by using a dynamic veto that takes advantage of the reconstructed track direction to compute the probability of a muon producing a similar signature.
Even neglecting the muon background, a through-going track sample will inevitably be overwhelmed by atmospheric neutrino backgrounds below $\SI{200}\TeV$.
Only a sample that actively vetos atmospheric events can have sensitivity to the astrophysical flux below $\SI{200}\TeV$.
In addition to the high-purity Southern hemisphere track information, the selection also has better energy resolution than other track samples.
This improved energy resolution gives an analysis sensitivity to smaller-scale features in the energy spectrum that the Northern sky track samples cannot discern.

In addition to the event selections described above, many identifiers for tau neutrino events have been developed to find tau neutrino events in low-level data.
However, there are no known properties of $\nu_\tau$ CC events that allow them to be readily separated from muon backgrounds and other types of neutrino interactions, leading to only a handful of candidate tau events.
Even without clear cut tau events, these identifiers can still differentiate $\nu_\tau$ CC events from others on a statistical basis if the muon background is first reduced.
Thus, the most effective approach is to apply these identifiers to existing selections and statistically infer the tau neutrino flux properties, rather than trying to separate the events outright.
Such identifiers use different methodologies, but all hinge on the properties of a particular class of $\nu_\tau$ CC events.
There are two physically distinct particle showers in this class of events, one from the initial neutrino DIS and the other from the tau decay.
By combining these event selections and identifiers, an analysis can test for structure and discrepancies in the different channels while also obtaining more precise measurements of the astrophysical flux that are less limited by systematic uncertainties.

\subsubsection{Cascade selection}
The cascade selection focuses on achieving a large effective area for cascade-like neutrino events while reducing muon backgrounds.
This is achieved through a combination of cascade identification, veto-like cuts, quality cuts, containment cuts, and machine learning based classification.
In an analysis of this high purity selection of cascade-like neutrino events the combined astrophysical flux of electron neutrinos and tau neutrinos can be precisely measured to lower energies.
The only confounding factors in this measurement are the proportion of neutral-current muon neutrino events, which can be constrained with other samples, and systematic variation to the contribution of atmospheric neutrinos to the sample.
Unfortunately we do not have the computational capacity or theoretical framework to accurately model muons from atmospheric showers in this type of event selection, so a number of cuts are made to remove sources of data-simulation disagreement that may arise from muons in the sample.
Similarly to the calculation techniques used for the analysis in this work, the cascade selection analysis computes the effect of accompanying muons with the older passing fraction calculation described in~\cite{Gaisser:2014bja}.
In this case the calculation of the passing fractions relies only on a single parameter, the median muon energy that triggers the detector.
The value of this parameter is chosen as a best-guess to be $\SI{100}\GeV$, which is verified by comparison to dedicated cosmic ray air-shower simulations.
While this treatment is sufficient for the current analysis of the cascade sample, more accurate calculations of the atmospheric suppression are desired for a global analysis so that possible differences between the different observation channels can be examined.

Absent the ability to compartmentalize the detector response to muons and neutrinos, the estimation of atmospheric backgrounds must rely on other methods than the calculation presented in \refsec{sec:passingfractions} or a more complicated extension of this.
Other available methods of modeling the interplay between related atmospheric muons and neutrinos and their combined effect on the event selection rely on air-shower simulations.
Air-shower simulations to this end have several downsides: they tend to be computationally expensive by comparison to signal simulation, must often be specialized for a particular event selection, are only available for a small subset of hadronic interaction models, and must be rerun for any hadronic interaction model changes.
For this reason it may prove difficult to obtain more accurate atmospheric neutrino predictions and add additional atmospheric systematic uncertainties to an analysis of the cascade sample.
An easier first step may be to use the less sensitive medium-energy starting event sample, for which accurate background estimations are more straightforward to obtain.

\subsubsection{Medium-energy starting events}
Much like the cascade selection, the medium-energy starting event selection also produces a cascade dominated sample that extends down to $\SI{1}\TeV$.
However, MESE achieves this by modifying the veto definitions of the HESE selection and adding additional cuts so that the charge threshold may be lowered.
The foremost cut is a charge dependent fiducial volume cut, where the fiducial volume size is reduced for lower charge events.
This smaller fiducial volume removes most of the lower energy muons, which are more likely to have a reconstructed vertex near the detector edge than in the center.

The MESE sample has similar properties to high-energy starting event selection in that only events with a contained interaction vertex are selected; it also retains the physical separation between veto and fiducial volume.
A similar set of calculations can be applied to estimate the MESE sample's backgrounds, and the selection is susceptible to the same systematics.
Primarily the addition of this selection allows us to explore the lower energies of the astrophysical spectrum between $\SI{10}\TeV$ and $\SI{100}\TeV$.
The lower energy range comes with its own challenges, though, these are primarily a larger data sample and higher atmospheric backgrounds.
To ensure accurate measurements with this increased sample size and background, we must ensure a higher degree of accuracy in the background estimates and potentially allow additional systematic variations.
In expanding our systematics considerations, the next logical step is the set of uncertainties related to cosmic rays and their production of neutrinos.
Thankfully, MCEq allows arbitrary cosmic ray model fluxes and compositions as input and can select from a range of hadronic models. With these tools, we can directly evaluate the effect of these systematics on the atmospheric neutrino flux.
At the same time, the improved passing fraction calculation leverages this same flexibility so that these systematics may be taken into account for accompanying muons as well.

\subsubsection{Northern sky through-going tracks and $\si\eV$ scale sterile neutrino searches}
The Northern sky through-going track selection is used to measure the astrophysical neutrino flux chooses well-reconstructed up-going track-like events; a boosted decision tree differentiates between neutrinos and misreconstructed muons near the horizon.
The boosted decision tree identification allows other quality cuts to be loosened, increasing the selection's effective area.
The larger effective area is optimal for an initial measurement of the astrophysical neutrino flux as much of the sensitivity comes from the high-energy tail where there are few events overall.
One difficulty with analyzing this type of sample is that the astrophysical contribution is buried beneath the atmospheric flux at lower energies.
Combined with the limited energy resolution of tracks, this makes it difficult to discern features in the energy spectrum beyond power-law-like behavior, especially below $\SI{100}\TeV$.
To make an accurate measurement of the astrophysical spectrum with this sample at lower energies, good data-simulation agreement and a detailed systematic treatment of the atmospheric background are necessary.
A reasonable way to achieve this is through more stringent quality cuts, which will reduce the effective area but potentially remove events that are more poorly modeled by simulation.
Similar steps have been taken in the sample used for $\si\eV$ scale sterile neutrino searches, but at lower energies.

Another similar sample, which does not have a particular name, is used to search for $\si\eV$ scale sterile neutrinos.
This particular search is a shape analysis that relies on percent-level differences in the atmospheric data compared to the standard model expectation.
Additionally, it is necessary to account for many potential confounding systematics.
As a result, the sample has sub-percent-level agreement between data and simulation, and a detailed treatment of the relevant systematics.
This precision is achieved by strict quality cuts for the event reconstructions and limiting the zenith range to avoid the horizon.
These same features are desirable in a global analysis of the astrophysical neutrino flux, so it may be easier to use this sample instead of the track sample currently used for astrophysical measurements.
The main caveat is that these optimizations made for lower energies may not necessarily hold at higher energies where the sample is sensitive to the astrophysical flux.

\subsubsection{Enhanced starting track event selection}
The two muon neutrino samples described above cover only the Northern hemisphere, so we would like to include a track sample with sensitivity in the Southern sky.
The enhanced starting track event selection (ESTES) works by taking the reconstructed event directions, computing the probability that a muon could sneak through the detector region behind the event vertex, and rejecting events where this probability is high.
This method is similar to veto techniques as a sufficiently detailed veto definition that depends on the event's properties could produce the same event selection.
With ESTES, the main challenge for background estimation is accounting for the interplay between muon and neutrino event properties and how they may conspire to accept or reject the event.
A sufficiently detailed $P_{\rm light}$ that depends on both muon and neutrino properties can account for this and allow us to use the calculation in \refsec{sec:passingfractions} for background estimation.
Although ESTES's effective area is smaller than other selections when all directions are considered, it is unique in its high astrophysical purity for down-going $\si\TeV$ track events.

In addition to adding track information in the Northern sky, ESTES also has better energy resolution than other track samples.
For the starting tracks that ESTES selects the initial hadronic cascade is observed, this additional information constrains the possible neutrino energies more than through-going track observations.
The improved energy resolution will allow an analysis ESTES to be more sensitive to features in the energy spectrum of astrophysical neutrinos.

\subsection{Analysis improvements}
Following \reffig{fig:analysis_design}, we can examine what components of the analysis may be improved.
In the previous subsection, we have already added additional samples, improving the ``event selection''.
Event reconstruction can be improved, but this topic has not been the focus of this work, so we will ignore it in this discussion.
The other interesting aspects that can be improved lie in the MC generation, likelihood function, and reweighting; in fact, the three are intimately related.
Signal estimation is relatively optimal at this stage of IceCube's operation apart from some unmodelled detector and ice systematics.
On the other hand, background estimation remains challenging and is susceptible to a wider array of systematic effects.
Background estimation can be broken down into two categories, the direct contribution of muons to the event selection and the effect of related muons on the acceptance of atmospheric neutrinos.

\subsubsection{Atmospheric muons}
For muons that directly contribute to observations, the main problem is the large number of muons in proportion to neutrinos.
We expect the detection of approximately $10^6$ more muons than neutrinos, based purely on the interaction cross section.
Assuming the simulation of a single muon or neutrino event incurs a similar computational cost, the simulation of background muons is vastly more expensive than the simulation of neutrinos for equivalent periods of time.
The event selections must also be extremely efficient at rejecting muons to measure the neutrino flux, so for all the expensive background estimation we can muster, there is often very little information regarding the distribution of muon backgrounds in the sample.
This shortcoming is exemplified in \reffig{fig:muons}, which shows the expected distribution of muons in the HESE selection.
Most of the bins in this figure are empty, and bins with simulated events only contain a handful.
This situation indicates inadequate simulation, and the aforementioned plot is a projection that reduces the number of bins by approximately a factor of three.
Clearly, this situation can be improved.
The two main ways to improve upon this without orders of magnitude more computing power are to improve simulation efficiency and to make better use of available information.

On simulation efficiency, the solution lies in generating events that are more likely to pass the event selection.
A technique to improve simulation efficiency has previously been employed in lower energy IceCube neutrino samples.
The goal is to preferentially generate events more likely to pass the event selection or equivalently reject events unlikely to pass the event selection before computational costs are incurred.
The distribution of event properties at an intermediate level of the event selection is estimated using a small simulation sample and a kernel density estimator.
Once the distribution of events in energy, direction, and impact position is determined, a larger sample is generated with this as the new generation distribution.
This approach lowers the computational resources dedicated to events that do not pass the event selection and leads to a larger sample at the final selection level.
This technique works well if the event selection has multiple levels and a reasonable distribution of muons can be obtained at some point.

However, veto techniques are too efficient at removing these backgrounds with the first cut for this to work.
For these cases, we can still attempt to bias the simulation generation distributions, but we must think more carefully about the kinds of events that pass the veto.
For a neutrino event, the relevant portion of the veto is a subspace of a cylindrical region behind the interaction vertex, effectively the region near where the event enters the detector.
Light detected in this region depends on the accompanying muon's stochastic energy losses and how close these energy losses are to the optical modules.
These energy losses are determined after the muon propagation stage, which happens to be computationally inexpensive.
Little computational cost has been incurred at this stage, so it is effective to bias the simulation chain here.
Knowing the energy losses, the expected number of photons to be observed in the veto region can be estimated.
From this estimation, the events are probabilistically rejected.
This probabilistic rejection is then accounted for in the weighting scheme.

Once we have improved estimates of the muon distributions from biased simulation, the task remains to better use the information in the analysis.
With the current treatment, empty bins in the muon distribution are poorly modeled from the likelihood perspective as the possibility of a non-zero expectation is not entertained.
The development of methods to treat these empty bins will improve our statistical description of the muon background information we have available, and remove existing bias from our results.
Some suggestions on this topic have been made, but so far, none are simultaneously satisfactory in their modeling of the per bin expectation and the corresponding uncertainty.
Possible solutions include nested models of the binned parameter space, kernel density estimation of the distribution with corresponding uncertainty estimation, and hierarchical modeling of the physical distributions and selection efficiencies.
Better treatment of this problem will be vital as we attempt to measure features in more background dominated regions.
However, more work on this topic is needed before we can arrive at a satisfactory solution.

\subsubsection{Atmospheric neutrinos}
As shown in the reweighting definitions, the atmospheric neutrino expectations depend on the neutrino flux and passing fraction calculations.
Both the flux model and the passing fractions are susceptible to systematic effects in the cosmic ray flux and hadronic interactions.
Thankfully it is possible to model these through a combination of the software packages MCEq and nuveto for the flux and passing fractions, respectively.
Modeling of the hadronic interaction uncertainties can be rather complex, with modeling and calculation methods varying wildly.

So far, a rough scheme has been developed to describe the hadronic model uncertainties by Barr et. al.~\cite{Barr:2004br}, which breaks the parameter space into rectangular box regions and assigns uncertainties to each region based on experimental measurements.
The chosen box region parameters can then be varied independently according to their prescribed uncertainties.
It is possible to develop better uncertainty treatments with the available data, but these have yet to be constructed by analyzers.
The Barr scheme for hadronic model modifications has been implemented in MCEq so that these modifications can be translated to variations in the neutrino flux and passing fractions with our available tools.
Ideally, we would account for hadronic model variations by allowing these box scaling parameters to vary according to known uncertainties, thereby modifying the background neutrino flux.
This method is implemented in IceCube's search for $\si\eV$ scale sterile neutrinos as a first-order approximation, where the gradient of the neutrino flux is computed with respect to the Barr parameters, and the atmospheric flux is allowed to vary linearly.
We would hope to apply this same technique to the atmospheric flux and the passing fractions simultaneously in the next generation of analyses.
Updating to better treatments of the uncertainty as they become available, and considering higher-order variations may be needed as we incorporate more data into the analysis.

A wide variety of cosmic ray flux models currently exist based on a wealth of measurements.
Unfortunately, most cosmic ray experiments cannot differentiate between different nuclei on the level of a single nucleon.
For this reason, many models of the cosmic ray flux will group various nuclei into between three and five mass groups, amongst which the spectrum may differ.

Fortunately, these models mostly follow a similar generic prescription that is so far capable of describing the observed data.
The cosmic ray spectrum is approximately a power law for any narrow energy range, with changes in the spectral index at the ``knee'', ``second knee'', and ``ankle''.
This spectral structure motivates a multi-component functional form common to many CR flux models, where each component is power law with a cutoff,
\begin{linenomath}
	\begin{equation}
	\frac{d^2\Phi_m}{dEd\Omega}\left(E_m\right) = \sum_i^{n_m} a_{i,m} E_m^{-b_{i,m}} e^{-E/c_{i,m}}.
	\end{equation}
	\label{eq:cr_spectrum}
\end{linenomath}
In this parameterization, the flux for each mass group $m$ depends on the normalizations $a_{i,m}$, spectral indices $b_{i,m}$, and cutoff energies $c_{i,m}$ of the different components.
Most models are constructed using similar basis functions, the parameters of which are then fit to some subset of the available cosmic ray data.
The same exercise can be performed where a likelihood connects the cosmic ray model parameters and the available data, but with MCEq we can predict the neutrino flux for any arbitrary cosmic ray model.
With this connection, we can construct a combined analysis of the neutrino and cosmic ray data, thereby accounting for cosmic ray uncertainties in the most direct manner possible.
This setup has some computational limitations as MCEq is slower than other steps in the likelihood evaluation for current analyses, but some possible workarounds limit the extra computation needed for each evaluation.

\subsubsection{Detector systematics}
One of the largest sources of systematic uncertainty in IceCube directional reconstructions is the ice properties.
Measurements of the ice properties that use different sources of information tend to be in tension with one another, and so far no clear explanation has surfaced.
A newer approach that models the ice as a birefringent material has shown promise in relieving some of these tensions, but this work is still in progress.
Currently, the bulk ice systematics are modeled in analyses as overall modifications to the ice properties, however, shape uncertainties in the depth dependent ice properties have been neglected.
Uncertainties on individual DOM properties, including the local ice effects, are either neglected or treated with a global modification.
It is possible to account for all of these uncertainties but the large number of additional parameters makes the current treatment with discrete simulation sets for each parameter variation untenable.
An alternate approach is given in~\cite{Aartsen:2019jcj}, where a single simulation set is produced in which the detector systematic parameters are randomly chosen according to prior distributions for small bunches of events.
After the generation of this simulation set, it is possible to estimate the effect of each parameter on the per bin expectation linearly.
This treatment has performed well for IceCube's search for $\si\eV$ scale sterile neutrinos and should be applied to future analyses of these samples to more completely account for the detector uncertainties.

\section{Final remarks}
IceCube has opened the door to a new era of astrophysical measurements with its observations of astrophysical neutrinos.
Initial measurements of the astrophysical neutrino flux have reaffirmed our understanding of neutrino physics while simultaneously challenging our expectations.
There is still much to learn from astrophysical neutrinos and much to be understood about even our current data.
The work presented here focuses on improving the analysis of IceCube's astrophysical data, with several new techniques developed in the process.
Such new techniques will be vital as we move into an era of precision measurements, in addition to the suggested analysis improvements described above.
Beyond these improvements, next generation detectors will also be vital to answering the questions raised in recent years.
A low energy extension to IceCube will help to pin down properties of the ice and the neutrino oscillation parameters.
The planned high-energy extension to IceCube will increase the effective area of IceCube above $\SI{10}\TeV$ by a factor of 10, allowing even more precise measurements of astrophysical neutrinos.
Finally, a partner radio array will be capable of observing neutrino interactions at even higher energies, complimenting the IceCube measurements.

Through the combination of these efforts I have high hopes for future of neutrino astrophysics.

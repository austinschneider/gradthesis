\chapter{Conclusions and next steps\label{chapter:conclusions}}
\section{Analysis conclusions\label{sec:analysis_conclusions}}
\begingroup
\graphicspath{{results/HESE_Final_Paper/}}
\input{results/HESE_Final_Paper/sections/conclusions}
\endgroup
\FloatBarrier

\section{The next generation}
The analysis presented here updated previous studies of the HESE sample, and is only a small part of the picture for astrophysical neutrinos and the mystery of cosmic ray origin.
However, the techniques and solutions developed here remain broadly applicable to future analyses, and will enable more precise measurements as more data is accumulated and additional samples are developed.

From the perspective of analyzing the astrophysical neutrino flux the next objectives are to expand the energy range and flavor information of the analysis, and to simultaneously examine these different channels for additional structure or differences between them.
Some analyses have already examined these channels, for instance samples that look at muon neutrinos from the Northern hemisphere between $\SI{100}\GeV$ and $\SI{10}\PeV$ or cascade focused samples that extend down to $\sim\SI{1}\TeV$.
However, the separate analyses of these samples have either lacked extended treatments of systematics, lacked enough data to make significant conclusions, or are too disparate for meaningful comparison between each other.
Further investigation of the astrophysical neutrino flux will require the simultaneous analysis of all these data samples, and should expand upon the techniques developed for this work.
Many of these samples were developed with the initial measurement of the astrophysical neutrino flux in mind.
However, as more data becomes available the precision of these analyses will no longer rely on maximizing the acceptance of the selection.
Instead, we will be able to focus on the analysis accuracy in an attempt to understand the differences we have observed between the various astrophysical neutrino measurement channels, even if this comes at the cost of some precision.

\subsection{Data samples}
The cascade data channel is able to achieve low background rates in all directions because it is unlikely for muons to produce cascade-like detector signatures, even at lower energies.
The HESE selection as a cascade dominated sample takes advantage of this in part, but is not able to cover lower energies.
With additional selection techniques a high purity sample of cascade-like neutrino events that extends down to $\SI{1}\TeV$ can be obtained.
Two IceCube event selections have achieved this, namely, the ``medium-energy starting event selection'' (MESE) and the ``cascade selection''.
These cascade dominated selections are largely comprised of $NC$, $\nu_e$ CC, and a sub-population of $\nu_\tau$ CC events.
In the case of the MESE selection, some $\nu_\mu$ CC events are also present in the sample.
These samples cover the entire sky, enabling the combined $\nu_e$ and $\nu_tau$ flux to be accurately measured in both hemispheres, although finer grained angular studies are limited by poor directional resolution.
A major advantage of these samples is the higher energy resolution of cascade events, which enables more detailed studies of the neutrino energy spectrum.

Beyond cascades, we hope to incorporate similar information from $\nu_\mu$ CC interactions.
This is available through the different track dominated event selections.
Two IceCube selections use a similar approach to obtain a sample of high quality through-going muon neutrino tracks, namely by restricting selected events to tracks known to be up-going.
These selections are those used for the Northern hemisphere astrophysical neutrino flux measurement and the search for $\si\eV$ scale sterile neutrinos.
Between these two selections the primary difference lies in the specifics of the selection cuts; the astrophysical measurement sample uses a boosted decision tree, while the sterile search sample uses only hand tunes straight cuts.
This choice stems from a difference in motivation for the samples.
The astrophysical sample aims to maximize effective area and angular range, while the sterile search sample aims for the high quality reconstruction and maximum data-simulation agreement.
These samples only cover the Northern hemisphere, but enable constraints on the energy and angular distribution of muon neutrinos within that range.
One disadvantage of these samples is their limited energy resolution that stems from the challenges of inferring muon energies.
Even under the assumption of an isotropic 1:1:1 astrophysical flux, there are benefits to including one of these samples in the analysis.
These samples extend between $\SI{100}\GeV$ and $\SI{10}\PeV$, and below a reconstructed energy of $\sim\SI{20}\TeV$ the flux is dominated by atmospheric neutrinos.
This large population of atmospheric neutrinos can help to pin down the unknown values of the atmospheric systematic parameters, thereby reducing uncertainty in the astrophysical measurements even for other samples.

Finally, to examine $\nu_\mu$ CC events in the Southern hemisphere we must include a sample that suppresses muon backgrounds significantly and maintains a large effective area for track-like neutrino events.
The enhanced starting track event selection (ESTES) fills this role by using a dynamic veto that takes advantage of the reconstructed track direction to compute the probability of a muon producing a similar signature.

\subsubsection{Cascade selection}
The cascade selection focuses on achieving a large effective area for cascade-like neutrino events while reducing muon backgrounds.
This is achieved through a combination of cascade identification, veto-like cuts, quality cuts, containment cuts, and machine learning based classification.
In an analysis of this high purity selection of cascade-like neutrino events the combined astrophysical flux of electron neutrinos and tau neutrinos can be precisely measured to lower energies.
The only confounding factors in this measurement are the proportion of neutral-current muon neutrino events, which can be constrained with other samples, and systematic variation to the contribution of atmospheric neutrinos to the sample.
Unfortunately we do not have the computational capacity or theoretical framework to accurately model muons from atmospheric showers in this type of event selection, so a number of cuts are made to remove sources of data-simulation disagreement that may arise from muons in the sample.
Similarly to the calculation techniques used for the analysis in this work, the cascade selection analysis computes the effect of accompanying muons with the older passing fraction calculation described in~\cite{Gaisser:2014bja}.
In this case the calculation of the passing fractions relies only on a single parameter, the median muon energy that triggers the detector.
The value of this parameter is chosen as a best-guess to be $\SI{100}\GeV$, which is verified by comparison to dedicated cosmic ray air-shower simulations.
While this treatment is sufficient for the current analysis of the cascade sample, more accurate calculations of the atmospheric suppression are desired for a global analysis so that possible differences between the different observation channels can be examined.

Absent the ability to compartmentalize the detector response to muons and neutrinos, the estimation of atmospheric backgrounds must rely on other methods than the calculation presented in \refsec{sec:passingfractions} or a more complicated extension of this.
Other available methods of modeling the interplay between related atmospheric muons and neutrinos and their combined effect on the event selection rely on air-shower simulations.
Air-shower simulations to this end have several downsides: they tend to be computationally expensive by comparison to signal simulation, must often be specialized for a particular event selection, are only available for a small subset of hadronic interaction models, and must be rerun for any hadronic interaction model changes.
For this reason it may prove difficult to obtain more accurate atmospheric neutrino predictions and add additional atmospheric systematic uncertainties to an analysis of the cascade sample.
An easier first step may be to use the less sensitive medium-energy starting event sample, for which accurate background estimations are more straightforward to obtain.

\subsubsection{Medium-energy starting events}
Much like the cascade selection, the medium-energy starting event selection also produces a cascade dominated sample that extends down to $\SI{1}\TeV$.
However, MESE achieves this by modifying the veto definitions of the HESE selection and adding additional cuts so that the charge threshold may be lowered.
The foremost of these is a charge dependent fiducial volume cut, where the fiducial volume size is reduced for lower charge events.
This smaller fiducial volume removes most of the lower energy muons, which are more likely to have a reconstructed vertex near the detector edge than in the center.

The MESE sample has similar properties to high-energy starting event selection in that only events with a contained interaction vertex are selected, it also retains the physical separation between veto and fiducial volume.
A similar set of calculations can be applied to estimate the backgrounds of the MESE sample, and the selection is susceptible to the same systematics.
Primarily the addition of this selection allows us to explore the lower energies of the astrophysical spectrum between $\SI{10}\TeV$ and $\SI{100}\TeV$.
The lower energy range comes with its own challenges though, mainly a much larger data sample and larger atmospheric backgrounds.
To ensure accurate measurements with this increased sample size and background we must ensure a higher degree of accuracy in the background estimates and potentially allow for additional systematic variations.
In the expansion of our systematics consideration a next logical step is the set of uncertainties related cosmic rays and their production of neutrinos.
Thankfully, MCEq allows arbitrary cosmic ray model fluxes and compositions as input, and has a range of hadronic models to select from. The effect of these systematics on the atmospheric neutrino flux can then be directly evaluated.
At the same time, the improved passing fraction calculation leverages this same flexibility so that these systematics may be taken into account for accompanying muons as well.

\subsubsection{Northern sky through-going tracks and $\si\eV$ scale sterile neutrino searches}
The Northern sky through-going track selection is used to measure the astrophysical neutrino flux chooses up-going track-like events that are well reconstructed; a boosted decision tree is also used to differentiate between neutrinos and mis-reconstructed muons near the horizon.
The boosted decision tree identification allows other quality cuts to be loosened, increasing the effective area of the selection.
This is optimal for an initial measurement of the astrophysical neutrino flux as much of the sensitivity comes from the high-energy tail where there are few events overall.
One difficulty with analyzing this type of sample is that the astrophysical contribution is buried beneath the atmospheric flux at lower energies.
Combined with the limited energy resolution of tracks, this makes it difficult to discern features in the energy spectrum.
To make an accurate measurement of the astrophysical spectrum with this sample at lower energies, good data-simulation agreement and a detailed systematic treatment of the atmospheric background are necessary.
A reasonable way to achieve this is through more stringent quality cuts, which will reduce the effective area but potentially remove events that are more poorly modeled by simulation.
Similar steps have been taken in the sample used for $\si\eV$ scale sterile neutrino searches, but at lower energies.

Another similar sample, which does not have a particular name, is used in the search for $\si\eV$ scale sterile neutrinos.
This particular search relies on percent-level differences in the data as compared to the standard model expectation.
Additionally, there are many potential confounding systematics that were necessary to account for.
As a result, the sample has sub-percent-level agreement between data and simulation, and a detailed treatment of the relevant systematics.
This is achieved by strict quality cuts for the event reconstructions, and limiting the zenith range to avoid the horizon.
These same features are desirable in a global analysis of the astrophysical neutrino flux, so it may be easier to use this sample as opposed to the track sample currently used for astrophysical measurements.
The main caveat is that these optimizations have been made for lower energies than those relevant for the astrophysical flux, and so they may not necessarily hold at higher energies.

\subsubsection{Enhanced starting track event selection}
The two muon neutrino samples described above cover only the Northern hemisphere, so we would like to include a track sample with sensitivity in the Southern sky.
The enhanced starting track event selection (ESTES) works by taking the reconstructed event directions, computing the probability that a muon could sneak through the detector region behind the event vertex, and rejecting events where this probability is high.
This is similar to veto techniques as a sufficiently detailed veto definition that depends on the properties of the event could produce the same event selection.
With ESTES the main challenge for background estimation is accounting for the interplay between muon and neutrino event properties and how they may conspire to have the event accepted or rejected.
A sufficiently detailed $P_{\rm light}$ that depends on both muon and neutrino properties can account for this and allow us to use the calculation in \refsec{sec:passingfractions} for background estimation.
Although the effective area of ESTES is smaller than other selections when all directions are considers, it is uniquely able to obtain a relatively pure sample of astrophysical neutrinos in the Southern hemisphere that extends down to a few $\si\TeV$.

Selections mentioned above cover the categories of cascades and tracks across a wider range of neutrino energies and encompass the entire sky.
Finally, the task remains to include information about tau neutrinos.
There are not any known properties of $\nu_\tau$ CC events that allow them to be readily separated from muon backgrounds and other types of neutrino interactions.
However, identifiers have been constructed to differentiate $\nu_\tau$ CC events from others assuming the events do not originate from the muon background.
Thus, the most effective approach is to apply these identifiers to existing selections and statistically infer the properties of the tau neutrino flux rather than trying to separate the events outright.
Such identifiers use different methodologies but all hinge on the properties of a particular class of $\nu_\tau$ CC events where there are two physically distinct particle showers in the event, one from the initial neutrino DIS and the other from the tau decay.

By combining these event selections and identifiers an analysis can be constructed that is able to test for structure and discrepancies in the different channels while also obtaining more precise measurements of the astrophysical flux that are less limited by systematic uncertainties.

\subsection{Analysis improvements}
Following \reffig{fig:analysis_design} we can examine what components of the analysis may be improved.
In the previous subsection we have already added additional samples, improving the ``event selection''.
Event reconstruction can be improved, but this topic has not been the focus of this work, and so we will ignore it for the moment.
The other interesting aspects that can be improved lie in the MC generation, likelihood function, and reweighting; in fact the three are related to one another.
Signal estimation is relatively optimal at this stage of IceCube's operation apart from some unmodeled detector and ice systematics.
Background estimation on the other hand remains challenging and is susceptible to a wider array of systematic effects.
This can be broken down into two categories, the direct contribution of muons to the event selection and the effect of related muons on the acceptance of atmospheric neutrinos.

\subsubsection{Atmospheric muons}
For muons that directly contribute to observations the main problem is the huge number of muons in proportion to neutrinos.
Based purely on interaction cross section the detection of approximately $10^6$ more muons than neutrinos is expected.
Assuming the simulation of a single muon or neutrino event incurs similar computational cost, the simulation of background muons is vastly more expensive than simulation of neutrinos for equivalent periods of time.
The event selections must also be extremely efficient at rejecting muons to measure the neutrino flux, so for all the expensive background estimation we can muster there is often very little information regarding the muon background distribution in the sample.
This is exemplified in \reffig{fig:muons} which shows the expected distribution of muons in the HESE selection.
Most of the bins in this figure are empty, indicating inadequate simulation, and this a projection that reduces the number of bins by approximately a factor of three.
Clearly this situation can be improved.
The two main ways to improve upon this without orders of magnitude more computing power are to improve simulation efficiency, and to make better use of available information.

On simulation efficiency, the trick lies in generating events that are more likely to pass the event selection.
A technique has been employed in lower energy samples where the distribution of injected event parameters at some intermediate level of the event selection is estimated using a small simulation sample and a kernel density estimator.
Once the distribution of events in energy, direction, and impact position is determined, a larger sample is generation with this as the new generation distribution.
This lowers the amount of computational resources dedicated to events that do not pass the event selection, and leads to a larger sample at the final selection level.
This technique works well if the event selection has multiple levels and a reasonable distribution of muons can be obtained at some point.
However, veto techniques are too efficient at removing these backgrounds with the first cut for this to work.
For these cases we can still attempt to bias the simulation generation distributions, but we must think more carefully about the kinds of events that pass the veto.
For a neutrino event, the relevant portion of the veto is a subspace of a cylindrical region behind the interaction vertex, where the particular subspace depends on the selection.
Light detected in this region depends on the stochastic energy losses of the accompanying muon, and how close these energy losses are to the optical modules.
The events can then be biased according to how close these energy losses come to the optical modules.
At the generation level this can be done by examining the path of the muon with respect to the detector geometry.
However, we can also bias the event sample immediately after muon propagation (which is computationally cheap), and before photon propagation (which is expensive), when the information about the energy losses is available.
Knowing the energy losses, we can estimate the expected number of photons to be observed in the veto region and probabilistically reject events on this basis.
This probabilistic rejection can then be accounted for in the weighting scheme.

Once we have improved estimates of the muon distributions from biased simulation, the task remains to make better use of the information in the analysis.
At the moment, empty bins in the muon distribution are poorly modeled from the likelihood perspective as the possibility of a non-zero expectation is not entertained.
The development of methods to treat these empty bins will improve our statistical description of the muon background information we have available, and remove existing bias from our results.
Some suggestions on this topic have been made but so far none are simultaneously satisfactory in their modeling of the per bin expectation and the corresponding uncertainty.
Possible solutions include nested models of the binned parameter space, kernel density estimation of the distribution with corresponding uncertainty estimation, and hierarchical modeling of the physical distributions and selection efficiencies.
Better treatment of this problem will be important as we attempt to measure features in more background dominated regions.

\subsubsection{Atmospheric neutrinos}
As shown in the reweighting definitions the atmospheric neutrino expectations depend on the neutrino flux model and the passing fraction calculations.
Both the flux model and the passing fractions are susceptible to systematic effects in the cosmic ray flux and hadronic interactions.
Thankfully it is possible to model these through a combination of MCEq and nuveto for the flux and passing fractions respectively.
Modeling of the hadronic interaction uncertainties can be rather complex with methods and calculation methods vary wildly.

So far a rough scheme has been developed to describe the hadronic model uncertainties by Barr et. al.~\cite{Barr:2004br}, that breaks the parameter space into rectangular box regions and assigns uncertainties to each region based on experimental measurements.
Better uncertainty treatments are possible with available data, but have yet to be constructed by analyzers.
The Barr scheme for hadronic model modifications has been implemented in MCEq, and so these modifications can be translated to variations in the neutrino flux and passing fractions with our available tools.
Ideally we would account for hadronic model variations by allowing these box scaling parameters to vary according to known uncertainties, thereby modifying the background neutrino flux.
This has been implemented in the MEOWS analysis as a first order approximation, where the gradient of the neutrino flux is computed with respect to the Barr parameters, and the atmospheric flux is allowed to vary linearly.
In the next generation of analyses we would hope to apply this same technique to the atmospheric flux and the passing fractions.
Updating to better treatments of the uncertainty as they become available, and considering higher order variations may be needed as we incorporate more data into the analysis.

A wide variety of cosmic ray flux models currently exist based on a wealth of measurements.
Most cosmic ray experiments are not able to differentiate between the different nuclei on the level of a single nucleon.
For this reason, many models of the cosmic ray flux will define between three and five mass groups between which the spectrum may differ.
Additionally, the cosmic ray spectrum is approximately a power law for any narrow energy range, with changes in the spectral index at the ``knee'', ``second knee'', and ``ankle''.
This spectral structure motivates a multi-component functional form common to many CR flux models, where each component is power law with a cutoff,
\begin{linenomath}
\begin{equation}
	\frac{d^2\Phi_m}{dEd\Omega}\left(E_m\right) = \sum_i^{n_m} a_{i,m} E_m^{-b_{i,m}} e^{-E/c_{i,m}}.
\end{equation}
	\label{eq:cr_spectrum}
\end{linenomath}
In this parameterization, the flux for each mass group $m$ depends on the normalizations $a_{i,m}$, spectral indices $b_{i,m}$, and cutoff energies $c_{i,m}$ of the different components.
Most models are constructed using similar basis functions, the parameters of which are then fit to some subset of the available cosmic ray data.
The same exercise can be performed where a likelihood connects the cosmic ray model parameters and the available data, but with MCEq we can predict the neutrino flux for any arbitrary cosmic ray model.
With this connection we can construct a combined analysis of the neutrino and cosmic ray data, thereby accounting for cosmic ray uncertainties in the most direct manner possible.
This setup has some computational limitations as MCEq is slower than other steps in the likelihood evaluation, but there are some possible workarounds to limit the extra computation needed for each evaluation.

\subsubsection{Detector systematics}
One of the largest sources of systematic uncertainty is the ice properties.
Measurements of the ice properties that use different sources of information tend to be in tension with one another, and so far no clear explanation has surfaced.
Modeling of the ice as a birefringent material has shown promise in relieving some of these tensions, but this work is still in progress.
Currently, the bulk ice systematics are modeled in analyses as overall modifications to the ice properties, however, shape uncertainties in the depth dependent ice properties have been neglected.
Uncertainties on individual DOM properties, including the local ice effects, are either neglected or treated with a global modification.
It is possible to account for all of these uncertainties but the large number of additional parameters makes the current treatment with discrete simulation sets for each parameter variation untenable.
An alternate approach is given in~\cite{Aartsen:2019jcj}, where a single simulation set is produced in which the detector systematic parameters are randomly chosen according to prior distributions for small bunches of events.
After the generation of this simulation set, it is possible to estimate the effect of each parameter on the per bin expectation linearly.
This treatment has performed well for the MEOWS analysis and should be applied to future analyses of these samples to more completely account for the detector uncertainties.